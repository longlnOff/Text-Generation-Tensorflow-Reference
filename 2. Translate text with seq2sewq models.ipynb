{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to a sequence-to-sequence (seq2seq) model for Spanish-to-English translation roughly based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025v5.pdf). The model uses an encoder-decoder architecture. The encoder is a bidirectional RNN that condenses the input sequence into a single vector, and the decoder is a RNN that expands that vector into a new sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](./images/notebook2_figure1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoder/decoder connected by attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this architecture is somewhat outdated, it is still a very useful project to work through to get a deeper understanding of seq2seq models and attention mechanisms before going on to Transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "tf.multiply(2, 3)\n",
    "clear_output()\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### The data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use dataset from [Anki](http://www.manythings.org/anki/). This dataset contains language translation pairs in the format: <br>\n",
    "`May I borrow this book? ¿Puedo tomar prestado este libro?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### Download and prepare the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset, here are the steps you need to take to prepare the data:\n",
    "1. Add a start and end token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 0: Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    context = np.array([pair[0] for pair in pairs])\n",
    "    target = np.array([pair[1] for pair in pairs])\n",
    "\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
     ]
    }
   ],
   "source": [
    "target_raw, context_raw = load_data(path_to_file)\n",
    "print(context_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "print(target_raw[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Cuando en Roma, haz como los romanos.' b'Soy un investigador privado.'\n",
      " b'Se est\\xc3\\xa1 haciendo mayor.'\n",
      " b'\\xc2\\xbfLe pedir\\xc3\\xadas a Tom que me llame de vuelta, por favor?'\n",
      " b'Tom no tuvo el valor de admitir que hab\\xc3\\xada cometido un error.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'When in Rome, do as the Romans do.' b\"I'm a private investigator.\"\n",
      " b'He is getting old.' b'Would you ask Tom to call me back, please?'\n",
      " b\"Tom didn't have the courage to admit that he had made a mistake.\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the goals of this tutorial is to build a model that can be exported as a tf.saved_model. To make that exported model useful it should take tf.string inputs and return tf.string outputs. All the text preprocessing happens inside the model. Mainly using a layers.TextVectorization layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### Standardization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents. The `tensorflow_text` package contains a unicode normalize operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?'\n",
      "b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode normalization will be the first step in the text standardization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### Text Vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c695a7c1db73d51cd02270fbcc6a5336958d25b354c79109e8794cb4835c69ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
